{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture:\n",
        "\n",
        "1. Embedding Layer:\n",
        "   - The input text data is converted into dense vectors using an embedding layer. Each word is represented as a fixed-size vector in a high-dimensional space.\n",
        "\n",
        "2. Bidirectional LSTM Layers:\n",
        "   - The embedding vectors are passed through Bidirectional LSTM (Long Short-Term Memory) layers. LSTM is a type of recurrent neural network (RNN) that can capture long-term dependencies in sequential data.\n",
        "   - The Bidirectional LSTM layers process the input sequences in both forward and backward directions, capturing information from past and future contexts.\n",
        "\n",
        "3. Dropout Layer:\n",
        "   - A dropout layer is added to prevent overfitting. Dropout randomly drops a fraction of input units to zero during training, which helps in regularization and improves generalization.\n",
        "\n",
        "4. Dense Layers:\n",
        "   - The output from the LSTM layers is fed into fully connected dense layers. These layers perform non-linear transformations on the input data, allowing the model to learn complex patterns.\n",
        "\n",
        "5. Output Layer:\n",
        "   - The final dense layer with a softmax activation function generates probabilities for the next word in the sequence. The word with the highest probability is selected as the predicted next word.\n",
        "\n",
        "6. Compilation:\n",
        "   - The model is compiled using the categorical cross-entropy loss function and the Adam optimizer. Categorical cross-entropy is commonly used for multi-class classification tasks, while Adam is an efficient optimizer for training neural networks.\n",
        "\n",
        "Training:\n",
        "   - The model is trained on the input sequences and corresponding labels (one-hot encoded). During training, the model adjusts its parameters (weights and biases) to minimize the loss function, i.e., the difference between predicted and actual outputs.\n",
        "\n",
        "Model Summary:\n",
        "   - The model summary provides a detailed overview of the layers, their output shapes, and the number of trainable parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "b71GlmvJ44kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "# Read the text data file\n",
        "data = open('poem.txt', encoding=\"utf8\").read()\n",
        "\n",
        "# Generate corpus by splitting the text into lines\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index)\n",
        "\n",
        "# Generate input sequences\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words+1, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words+1, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(predictors, label, epochs=50, verbose=1)\n"
      ],
      "metadata": {
        "id": "-l7tpJLLT9Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b80b3b-9f70-4aab-e47d-72ae677886ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 13, 100)           65600     \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 13, 300)           301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 13, 300)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 656)               66256     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 593456 (2.26 MB)\n",
            "Trainable params: 593456 (2.26 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "45/45 [==============================] - 10s 106ms/step - loss: 6.2842 - accuracy: 0.0388\n",
            "Epoch 2/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 5.9067 - accuracy: 0.0409\n",
            "Epoch 3/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 5.8321 - accuracy: 0.0430\n",
            "Epoch 4/50\n",
            "45/45 [==============================] - 4s 84ms/step - loss: 5.7834 - accuracy: 0.0451\n",
            "Epoch 5/50\n",
            "45/45 [==============================] - 4s 96ms/step - loss: 5.6600 - accuracy: 0.0529\n",
            "Epoch 6/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 5.4853 - accuracy: 0.0592\n",
            "Epoch 7/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 5.3587 - accuracy: 0.0585\n",
            "Epoch 8/50\n",
            "45/45 [==============================] - 5s 107ms/step - loss: 5.2172 - accuracy: 0.0599\n",
            "Epoch 9/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 5.0886 - accuracy: 0.0648\n",
            "Epoch 10/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 5.0171 - accuracy: 0.0641\n",
            "Epoch 11/50\n",
            "45/45 [==============================] - 4s 95ms/step - loss: 4.9043 - accuracy: 0.0698\n",
            "Epoch 12/50\n",
            "45/45 [==============================] - 4s 84ms/step - loss: 4.7981 - accuracy: 0.0796\n",
            "Epoch 13/50\n",
            "45/45 [==============================] - 3s 76ms/step - loss: 4.7139 - accuracy: 0.0860\n",
            "Epoch 14/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 4.6533 - accuracy: 0.0832\n",
            "Epoch 15/50\n",
            "45/45 [==============================] - 5s 107ms/step - loss: 4.5745 - accuracy: 0.0860\n",
            "Epoch 16/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 4.4753 - accuracy: 0.0944\n",
            "Epoch 17/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 4.3938 - accuracy: 0.1008\n",
            "Epoch 18/50\n",
            "45/45 [==============================] - 5s 106ms/step - loss: 4.3196 - accuracy: 0.1078\n",
            "Epoch 19/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 4.2371 - accuracy: 0.1149\n",
            "Epoch 20/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 4.1623 - accuracy: 0.1198\n",
            "Epoch 21/50\n",
            "45/45 [==============================] - 3s 77ms/step - loss: 4.1009 - accuracy: 0.1325\n",
            "Epoch 22/50\n",
            "45/45 [==============================] - 5s 103ms/step - loss: 4.0290 - accuracy: 0.1332\n",
            "Epoch 23/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 3.9634 - accuracy: 0.1367\n",
            "Epoch 24/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 3.8755 - accuracy: 0.1572\n",
            "Epoch 25/50\n",
            "45/45 [==============================] - 5s 107ms/step - loss: 3.8153 - accuracy: 0.1550\n",
            "Epoch 26/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 3.7666 - accuracy: 0.1642\n",
            "Epoch 27/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 3.6784 - accuracy: 0.1917\n",
            "Epoch 28/50\n",
            "45/45 [==============================] - 4s 82ms/step - loss: 3.6334 - accuracy: 0.1853\n",
            "Epoch 29/50\n",
            "45/45 [==============================] - 4s 97ms/step - loss: 3.5871 - accuracy: 0.1980\n",
            "Epoch 30/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 3.5157 - accuracy: 0.2051\n",
            "Epoch 31/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 3.4650 - accuracy: 0.2121\n",
            "Epoch 32/50\n",
            "45/45 [==============================] - 5s 106ms/step - loss: 3.4029 - accuracy: 0.2276\n",
            "Epoch 33/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 3.3447 - accuracy: 0.2213\n",
            "Epoch 34/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 3.2823 - accuracy: 0.2297\n",
            "Epoch 35/50\n",
            "45/45 [==============================] - 4s 91ms/step - loss: 3.2268 - accuracy: 0.2622\n",
            "Epoch 36/50\n",
            "45/45 [==============================] - 4s 90ms/step - loss: 3.1900 - accuracy: 0.2509\n",
            "Epoch 37/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 3.1285 - accuracy: 0.2678\n",
            "Epoch 38/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 3.0708 - accuracy: 0.2812\n",
            "Epoch 39/50\n",
            "45/45 [==============================] - 5s 104ms/step - loss: 3.0343 - accuracy: 0.2826\n",
            "Epoch 40/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 2.9757 - accuracy: 0.2988\n",
            "Epoch 41/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 2.9199 - accuracy: 0.3066\n",
            "Epoch 42/50\n",
            "45/45 [==============================] - 4s 97ms/step - loss: 2.8946 - accuracy: 0.3066\n",
            "Epoch 43/50\n",
            "45/45 [==============================] - 4s 81ms/step - loss: 2.8431 - accuracy: 0.3178\n",
            "Epoch 44/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 2.7964 - accuracy: 0.3425\n",
            "Epoch 45/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 2.7499 - accuracy: 0.3488\n",
            "Epoch 46/50\n",
            "45/45 [==============================] - 5s 106ms/step - loss: 2.7023 - accuracy: 0.3643\n",
            "Epoch 47/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 2.6825 - accuracy: 0.3770\n",
            "Epoch 48/50\n",
            "45/45 [==============================] - 3s 74ms/step - loss: 2.6137 - accuracy: 0.3763\n",
            "Epoch 49/50\n",
            "45/45 [==============================] - 5s 106ms/step - loss: 2.5790 - accuracy: 0.3925\n",
            "Epoch 50/50\n",
            "45/45 [==============================] - 3s 75ms/step - loss: 2.5282 - accuracy: 0.4094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save(\"NLP.h5\")"
      ],
      "metadata": {
        "id": "OhIhuZus3B3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c4c9e5-3849-4223-a3ff-d4c7e50ae40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the saved model\n",
        "\"\"\"\n",
        "from tensorflow.keras.models import load_model\n",
        "loaded_model = load_model(\"NLP.h5\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sGoR1bKI4NpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poem():\n",
        "\n",
        "  # Generate text\n",
        "  seed_text = \"The world\"\n",
        "  next_words = 100\n",
        "  output_text = \"\"\n",
        "\n",
        "  for _ in range(next_words):\n",
        "      token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "      token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "      predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "      output_word = \"\"\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == predicted:\n",
        "              output_word = word\n",
        "              break\n",
        "      seed_text += \" \" + output_word\n",
        "\n",
        "  return seed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjxnIdSK2xvl",
        "outputId": "8fa5a039-10e7-4beb-93b7-18fd448d83a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "The world to love to strangers of was me known of was me loud of do of do do do the mournful bleak bound rose erase do the stone rose erase of day do do this in the stone rose own can to do the stone hours erase of was do me now from me the stone rose erase beside the stone hours erase of was do do the mournful sea hours hours rose erase do the echoes hours erase do the echoes hours erase of was me loud rose of was me loud beside the stone rose rose erase of day\n"
          ]
        }
      ]
    }
  ]
}